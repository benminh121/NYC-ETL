# ğŸš• NYC_TAXI Data Pipeline ğŸš•
This project demonstrates a complete data pipeline built with **Apache Airflow**, **PySpark**, and **PostgreSQL**. The pipeline performs the following steps:

1. **Data Ingestion**: Moves Parquet files from the local system to an object storage service (**MinIO**).  
2. **Data Transformation**: Processes and transforms the data using **PySpark**, converting it into a structured format.  
3. **Data Storage**: Saves the transformed data to a **Delta Lake** for reliability and incremental updates.  
4. **Data Warehouse Integration**: Loads the processed data into a **PostgreSQL** Data Warehouse for analytics and reporting.  

This project showcases scalable, modern data engineering practices and integrates open-source tools to handle large-scale data efficiently.

# ğŸ“ Repository Structure

```shell
.
    â”œâ”€â”€ airflow/                                    /* airflow folder including dags,.. /*
    â”œâ”€â”€ batch_processing/
    â”‚   â””â”€â”€ datalake_to_dw.py                           /* ETL data from datalake to staging area /*
    â”œâ”€â”€ configs/                                    /* contain config files /*
    â”‚   â”œâ”€â”€ spark.yaml
    â”‚   â””â”€â”€ datalake.yaml
    â”œâ”€â”€ data/                                       /* contain dataset /*
    â”‚   â”œâ”€â”€ 2020/
    â”‚   â”œâ”€â”€ 2021/
    â”‚   â”œâ”€â”€ 2022/
    â”‚       â”œâ”€â”€ green_tripdata_2022-01.parquet
    â”‚       â”œâ”€â”€ green_tripdata_2022-02.parquet
    â”‚       â”œâ”€â”€ green_tripdata_2022-03.parquet
    â”‚       â”œâ”€â”€ ...
    â”‚       â”œâ”€â”€ yellow_tripdata_2022-01.parquet
    â”‚       â”œâ”€â”€ yellow_tripdata_2022-02.parquet
    â”‚       â”œâ”€â”€ yellow_tripdata_2022-03.parquet
    â”‚       â””â”€â”€ ...
    â”‚   â”œâ”€â”€ 2023/
    â”‚   â””â”€â”€ 2024/                                  /* run create connector */
    â”œâ”€â”€ imgs/
    â”œâ”€â”€ jars/                                       /* JAR files for Spark version 3.5.1 */
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ data/
    â”‚       â””â”€â”€ taxi_lookup.csv                             /* CSV file to look up latitude and longitude */
    â”‚   â”œâ”€â”€ extract_load.py                             /* upload data from local to 'raw' bucket (MinIO) */
    â”‚   â”œâ”€â”€ transform_data.py                           /* transform data to 'processed' bucket (MinIO) */
    â”‚   â””â”€â”€ convert_to_delta.py                         /* convert data parquet file from 'processed' to 'delta' bucket (MinIO) */
    â”œâ”€â”€ utils/                                     /* functions /*
    â”‚    â”œâ”€â”€ create_schema.py
    â”‚    â”œâ”€â”€ create_table.py
    â”‚    â”œâ”€â”€ postgresql_client.py                       /* PostgreSQL Client: create connect, execute query, get columns in bucket /*
    â”‚    â”œâ”€â”€ helper.py
    â”‚    â”œâ”€â”€ minio_utils.py                             /* Minio Client: create connect, create bucket, list parquet files in bucket /*
    â”œâ”€â”€ .env
    â”œâ”€â”€ .gitignore
    â”œâ”€â”€ airflow-docker-compose.yaml
    â”œâ”€â”€ docker-compose.yaml
    â”œâ”€â”€ Makefile
    â”œâ”€â”€ README.md
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ stream-docker-compose.yaml
```

# ğŸŒŸ System Architecture

<p align="center">
<img src="./imgs/SCR-20241118-tdaz.png" width=100% height=100%>

<p align="center">
    System Architecture
</p>

# ğŸš€ Getting Started

1.  **Clone the repository**:

    ```bash
    git clone https://github.com/benminh121/NYC-ETL
    ```

2.  **Start all infrastructures**:

    ```bash
    make run_all
    ```

    This command will download the necessary Docker images, create containers, and start the services in detached mode.

3.  **Setup environment**:

    ```bash
    conda create -n nyc python==3.12
    y
    conda activate nyc
    pip install -r requirements.txt
    ```

    Activate your conda environment and install required packages

4.  **Access the Services**:

    - Postgres is accessible on the default port `5432`.
    - MinIO is accessible at `http://localhost:9001`.
    - Airflow is accessible at `http://localhost:8080`.

5.  **Download Dataset**:
    You can download and use this dataset in here: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page

6.  **Download JAR files for Spark**:

    ```bash
    mkdir jars
    cd jars
    curl -O https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar
    curl -O https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar
    curl -O https://repo1.maven.org/maven2/org/postgresql/postgresql/42.4.3/postgresql-42.4.3.jar
    curl -O https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.2.1/spark-sql-kafka-0-10_2.12-3.2.1.jar
    ```